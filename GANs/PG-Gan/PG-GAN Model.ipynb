{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "befd71a4-9442-4d78-90d1-0264a8c9c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54f438f4-a5ae-45d9-9768-4a3582b3d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset Handler Class, as well as model saving and loading\n",
    "class CelebAHQDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = sorted(os.listdir(root_dir))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image\n",
    "    \n",
    "def get_transform(resolution):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((resolution, resolution)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "def get_data_loader(resolution):\n",
    "    batch_size = BATCH_SIZES[int(log2(resolution / 4))]\n",
    "    dataset = CelebAHQDataset(root_dir='data/celeba-hq/images/', transform=get_transform(resolution = resolution))\n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    return loader, dataset\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename):\n",
    "    print('=> Saving Checkpoint')\n",
    "\n",
    "    checkpoint = {'state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print('=> Loading Checkpoint')\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_file, map_location='cuda')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer']) # else 'optimizer'\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6bcf7bb-7a17-47a6-8846-228a671b51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Model architecture implementation \n",
    "# https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/ProGAN\n",
    "\n",
    "class WeightedConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
    "    \n",
    "class PixelNorm(nn.Module):\n",
    "    def  __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
    "    \n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n",
    "        super().__init__()\n",
    "        self.use_pn = use_pixelnorm\n",
    "        self.conv1 = WeightedConv2D(in_channels, out_channels)\n",
    "        self.conv2 = WeightedConv2D(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(in_channels=z_dim, out_channels=in_channels, kernel_size=4, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WeightedConv2D(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WeightedConv2D(in_channels=in_channels, out_channels=img_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([self.initial_rgb])\n",
    "\n",
    "        for i in range(len(factors) - 1):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i+1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n",
    "            self.rgb_layers.append(WeightedConv2D(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "    def fade_in(self, alpha, upscaled, generated):\n",
    "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
    "    \n",
    "    def forward(self, x, alpha, steps):\n",
    "        out = self.initial(x)\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(out)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode='nearest')\n",
    "            out = self.prog_blocks[step](upscaled)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.prog_blocks, self.rgb_layers= nn.ModuleList(), nn.ModuleList()\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors) -1, 0, -1):\n",
    "            conv_in_c = int(in_channels * factors[i])\n",
    "            conv_out_c = int(in_channels * factors[i -1])\n",
    "            self.prog_blocks.append(ConvBlock(in_channels=conv_in_c, out_channels=conv_out_c, use_pixelnorm=False))\n",
    "            self.rgb_layers.append(WeightedConv2D(in_channels=img_channels, out_channels=conv_in_c, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "        self.initial_rgb = WeightedConv2D(img_channels, in_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.final_block = nn.Sequential(\n",
    "            WeightedConv2D(in_channels=in_channels + 1, out_channels= in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WeightedConv2D(in_channels=in_channels, out_channels=in_channels, kernel_size=4, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WeightedConv2D(in_channels=in_channels, out_channels=1, kernel_size=1, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "    \n",
    "    def minibatch_std(self, x):\n",
    "        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "    \n",
    "    def forward(self, x, alpha, steps):\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "        \n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "        \n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf6ae2d-ce3e-41f1-bc6c-462ca71a50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WS-Loss with Gradient Penalty\n",
    "def wasserstein_loss(d_real, d_fake):\n",
    "    return torch.mean(d_fake) - torch.mean(d_real)\n",
    "\n",
    "def gradient_panelty(discriminator, real_samples, fake_samples, alpha, step, device):\n",
    "    batch_size, c, h, w = real_samples.shape\n",
    "    beta = torch.rand((batch_size, 1, 1, 1)).repeat(1, c, h, w).to(device)\n",
    "    interpolates = (beta * real_samples + ((1 - beta) * fake_samples.detach()))\n",
    "    interpolates.requires_grad = True\n",
    "    \n",
    "    d_interpolate = discriminator(interpolates, alpha, step)\n",
    "    fake = torch.ones(real_samples.size(0), 1).to(device)\n",
    "    gradients = torch.autograd.grad(outputs=d_interpolate, inputs=interpolates, grad_outputs=fake, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_panelty = ((gradients.norm(2, dim=1) -1) ** 2).mean()\n",
    "    return gradient_panelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fc10bbc-9c71-447b-98d1-540cd0efe2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainer(num_epochs, generator, discriminator, dataloader, dataset, optimizer_gen, optimizer_critic, z_dim, step, alpha, scaler_gen, scaler_critic, logging_interval, save_interval):\n",
    "    for epoch in range(num_epochs):\n",
    "        data_dict = {'Generator_losses':[],\n",
    "                'Discriminator_losses':[]}\n",
    "        \n",
    "        for batch_idx, real_images in enumerate(dataloader):\n",
    "            real_images = real_images.to(DEVICE)\n",
    "            cur_batch_size = real_images.shape[0]\n",
    "\n",
    "            z = torch.randn(cur_batch_size, z_dim, 1, 1).to(DEVICE)\n",
    "            \n",
    "            #Train Descriminator\n",
    "            with torch.cuda.amp.autocast():\n",
    "                fake_image = generator(z, alpha, step)\n",
    "                real_output = discriminator(real_images, alpha, step)\n",
    "                fake_output = discriminator(fake_image.detach(), alpha, step)\n",
    "                \n",
    "                #WS Loss with gradient panelty\n",
    "                gp = gradient_panelty(discriminator=discriminator, real_samples=real_images, fake_samples=fake_image, alpha=alpha, step=step, device=DEVICE)\n",
    "                w_loss = wasserstein_loss(d_real=real_output, d_fake=fake_output)\n",
    "                loss_critic = w_loss + LAMBDA_GP * gp + (0.001 * torch.mean(real_output ** 2))\n",
    "\n",
    "            optimizer_critic.zero_grad()\n",
    "            scaler_critic.scale(loss_critic).backward()\n",
    "            scaler_critic.step(optimizer_critic)\n",
    "            scaler_critic.update()\n",
    "\n",
    "            # Train Generator\n",
    "            with torch.cuda.amp.autocast():\n",
    "                gen_fake = discriminator(fake_image, alpha, step)\n",
    "                gen_loss = -torch.mean(gen_fake)\n",
    "            \n",
    "            optimizer_gen.zero_grad()\n",
    "            scaler_gen.scale(gen_loss).backward()\n",
    "            scaler_gen.step(optimizer_gen)\n",
    "            scaler_gen.update()\n",
    "        \n",
    "            alpha += cur_batch_size / (\n",
    "                 (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
    "            )\n",
    "            alpha = min(alpha, 1)\n",
    "            \n",
    "            if batch_idx % logging_interval == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}],Of Batch [{batch_idx}/{len(dataloader)}], \"\n",
    "                        f\"Genenerator Loss: {gen_loss.item():.4f}, Discriminator Loss: {loss_critic.item():.4f}\")\n",
    "           \n",
    "            if batch_idx % save_interval == 0:\n",
    "                 with torch.no_grad():\n",
    "                      z = torch.randn(16, z_dim, 1, 1).to(DEVICE)\n",
    "                      generated_imgs = generator(z, alpha, step).detach().cpu()\n",
    "                      save_image(generated_imgs, fp='data/generated_img_256/Resolution_'+ str(current_resolution) + '_epoch_'+ str(epoch) +'.png', nrow=4, normalize=True)\n",
    "\n",
    "            data_dict['Generator_losses'].append(gen_loss.item())\n",
    "            data_dict['Discriminator_losses'].append(loss_critic.item())\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65ff91eb-09a8-480f-820a-756bd82b7e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "=> Loading Checkpoint\n"
     ]
    }
   ],
   "source": [
    "#Hyper Parameters\n",
    "CUDA_DEVICE_NUM = 0\n",
    "DEVICE = torch.device(f'cuda:{CUDA_DEVICE_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "LR = 0.001\n",
    "BATCH_SIZES = [32, 32, 16, 16, 16, 16, 8, 4]\n",
    "PROGRESSIVE_EPOCHS = [10] * len(BATCH_SIZES)\n",
    "START_TRAIN_AT_IMG_SIZE = 256\n",
    "Z_DIM = 256\n",
    "IN_CHANNELS = 256\n",
    "LAMBDA_GP = 10\n",
    "MAX_RESOLUTION = 512\n",
    "factors = [1, 1, 1, 1, 1 / 2, 1 / 4, 1 / 8, 1 / 16, 1 / 32]\n",
    "\n",
    "\n",
    "gen = Generator(z_dim=Z_DIM, in_channels=IN_CHANNELS,img_channels=3).to(DEVICE)\n",
    "critic = Discriminator(in_channels=IN_CHANNELS, img_channels=3).to(DEVICE)\n",
    "\n",
    "optimizer_gen = optim.Adam(gen.parameters(), lr=LR, betas=(0.0, 0.99))\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=LR, betas=(0.0,0.99))\n",
    "\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "scaler_gen = torch.cuda.amp.GradScaler()\n",
    "scaler_critic = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a83f6-6660-4a86-ba3e-cf6c0584c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n",
    "\n",
    "epoch_counter = 0\n",
    "start_time = time.time()\n",
    "for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "    epoch_counter += num_epochs\n",
    "    current_resolution = 4 * 2 ** step\n",
    "    if current_resolution > MAX_RESOLUTION:\n",
    "        break\n",
    "    alpha = 1e-5\n",
    "    loader, dataset = get_data_loader(4 * 2 ** step) # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n",
    "    print(f\"Current image size: {4 * 2 ** step}\")\n",
    "\n",
    "    train = trainer(num_epochs=num_epochs, generator=gen, discriminator=critic, dataloader=loader, dataset=dataset, optimizer_gen=optimizer_gen,\n",
    "                    optimizer_critic= optimizer_critic, z_dim=Z_DIM, step=step, alpha=alpha, scaler_gen=scaler_gen, scaler_critic=scaler_critic, \n",
    "                    logging_interval=700, save_interval=2500)\n",
    "    step+=1\n",
    "    print(f\"Time elapsed: {(time.time() - start_time)/60:.2f} minutes\")\n",
    "\n",
    "    save_checkpoint(gen, optimizer_gen, filename=f'data/pro_gan_models/Checkpoint_Gen_Resolution_{current_resolution}_.pth')\n",
    "    save_checkpoint(critic, optimizer_critic, filename=f'data/pro_gan_models/Checkpoint_Critic_Resolution_{current_resolution}_.pth')\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Training finished. Total time: {(end_time - start_time)/60:.2f} minutes\")\n",
    "print(f\"Total epochs trained: {epoch_counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95f266-1fb0-486b-8f76-6239caae91d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generator and discriminator losses\n",
    "generator_losses = train['Generator_losses']\n",
    "discriminator_losses = train['Discriminator_losses']\n",
    "\n",
    "\n",
    "# Calculate the average loss per epoch or batch\n",
    "avg_generator_losses = [sum(generator_losses[:i+1]) / len(generator_losses[:i+1]) for i in range(len(generator_losses))]\n",
    "avg_discriminator_losses = [sum(discriminator_losses[:i+1]) / len(discriminator_losses[:i+1]) for i in range(len(discriminator_losses))]\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(avg_generator_losses, label='Average Generator Loss', color='blue')\n",
    "plt.plot(avg_discriminator_losses, label='Average Discriminator Loss', color='orange')\n",
    "plt.xlabel('Epochs or Batches')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9b8cd4d-a645-4705-893e-07558b8e3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(gen, steps, n=50):\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    \n",
    "    # Generate random latent vectors\n",
    "    latent_vectors = torch.randn(n, 256, 1, 1, device=DEVICE)\n",
    "    \n",
    "    # Generate examples\n",
    "    with torch.no_grad():\n",
    "        images = gen(latent_vectors, alpha, steps)\n",
    "    \n",
    "    # Post-process the generated images (optional)\n",
    "    images = (images * 0.5) + 0.5  # Scale images to [0, 1] range\n",
    "    \n",
    "    # Save the images\n",
    "    save_image(images, 'data/generated_examples.png', nrow=3)  # Save as a 3x3 grid\n",
    "    #plt.figure(figsize=(10, 10))\n",
    "    #grid = make_grid(images, nrow=3, normalize=True, scale_each=True)\n",
    "    #plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    #plt.axis('off')\n",
    "    #plt.show()\n",
    "    gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "344f634b-6ddd-43ce-9cef-f2f0609cad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_examples(gen, steps=int(log2(START_TRAIN_AT_IMG_SIZE / 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
